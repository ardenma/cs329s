import argparse
import requests
import logging
from time import perf_counter

import streamlit as st

from utils.datatypes import Query, Feedback, PredictionFeedback

class StreamlitFrontend():
    def __init__(self, args):
        self.server_endpoint = args.server_endpoint
        logging.info(f"Connecting to server at {self.server_endpoint}")

        if 'current_query_id' not in st.session_state:
            st.session_state['current_query_id'] = 0
        
        if 'current_response' not in st.session_state:
            st.session_state['current_response'] = None
        
        self.render()

    def render(self):
        st.title('Misinformation Detection App')
        st.text_input("Try a query below:", key="query", placeholder='Type query here!', on_change=self.send_query)
        self.render_blurb()
        if st.session_state.current_response is not None:
            self.render_result()
            self.render_full_response()
        self.render_feedback()
        st.markdown("Check out our github at https://github.com/ardenma/cs329s :)")

    def render_blurb(self):
        st.markdown("Hi and welcome to our **Misinformation Detection App**! Try submitting a query above, or click on the below box for more information about our app and model!")
        with st.expander(label= "More Information / Technical Details:", expanded=False):
            # st.markdown("Our approach consists of 3 main steps:")
            st.subheader("Summary of Approach:")
            st.markdown("1. Train a transformer model to generate embededings from query strings using data from the LIAR dataset, a collection of politifact statements labeled by their truthfulness.")
            st.markdown("2. Use this model to generate an embedding space of the LIAR dataset examples")
            st.markdown("3. Then, given a query we can generate and embedding, match it to the K-closest embeddings in our embedding space, \
                        and then utilize a voting based approach to generate inferences (e.g. if 2/3 votes are 'true' we will return the label true!)")
            st.markdown("See our github for the full source code: https://github.com/ardenma/cs329s")
            st.subheader("Model Information:")
            st.markdown("- For our embedding model we finetuned a DistilBERT (https://arxiv.org/pdf/1910.01108.pdf) model on the LIAR dataset")
            st.markdown("- For our prediction model, we used a KNN model with K=3 (breaking ties randomly) on the embedding space generated by our embedding model over the training examples in the LIAR dataset")
            st.subheader("Dataset Information:")
            st.markdown("- We used the LIAR dataset (https://huggingface.co/datasets/liar) for training and evaluating our model")
            st.markdown("- Because the LIAR dataset is quite challenging (highest accuracy reported in the [original paper](https://arxiv.org/pdf/1705.00648.pdf) was 27%) we used a **modified** version of the dataset where we combined the labels as follows: (**pants-fire**, **false**) -> **false**, (**barely-true**, **half-true**) -> **unsure**, (**mostly-true**, **true**) -> **true**")
            st.markdown("- All reported metrics are based off this label-joining scheme")

    def render_feedback(self):
        with st.container():
            st.header("Feedback:")
            st.text_input("Please leave some feedback!", key="text_feedback", placeholder='Type feedback here!', on_change=self.send_feedback)

    def render_result(self):
        resp = st.session_state['current_response']
        st.header(f"Results:")
        with st.container():
            col1, col2 = st.columns(2)
            col1.metric("Predicted Label:", resp['predicted_class'])
            if 'current_model_name' in st.session_state:
                col1.write(f"Current model and version: {st.session_state.current_model_name}:{st.session_state.current_model_version}")
                col1.write(f"Current model F1 score on 3 label LIAR dataset test split: {st.session_state.current_model_test_f1:.3f}")
            col1.button(label="request_model_info", on_click=self.request_model_info)

            with col2.form("feedback_form "):
                st.radio(label="Think we got something wrong? Please suggest a new class label!", key="user_suggested_label", options=["true", "false", "unsure"])
                st.form_submit_button(on_click=self.send_prediction_feedback)

        with st.container():
            col1, col2, col3  = st.columns(3)
            col1.metric("End-to-end Latency (ms)", f"{st.session_state.last_latency_ms:.3f}", f"{st.session_state.last_latency_delta_ms:.3f}") 
            col2.metric("Inference Latency (ms)", f"{st.session_state.last_server_latency_ms:.3f}", f"{st.session_state.last_server_latency_delta_ms:.3f}")
            col3.metric("Network Latency (ms)", f"{st.session_state.last_network_latency_ms:.3f}", f"{st.session_state.last_network_latency_delta_ms:.3f}" )

        st.header(f"Most similar statments:")
        st.write(resp['most_similar_examples'])

        st.header("Statement class labels and similarities:")
        for i in range(3):
            with st.container():
                col1, col2, col3 = st.columns(3)
                col1.write(resp['most_similar_examples'][i])
                col2.metric("Label", resp["example_classes"][i])
                col3.metric("Cosine Similarity", f"{resp['example_similarities'][i]:.3f}")
    
    def render_full_response(self):
        resp = st.session_state['current_response']
        st.header("Full API response:")
        with st.expander(label= "Click to expand!", expanded=False):
            st.write(resp)

    def send_query(self):
        query = Query(id=st.session_state.current_query_id, data=st.session_state.query).json()
        st.session_state.current_query_id += 1
        
        # Send query to server
        time_start = perf_counter()
        with st.spinner("Sending query to server. Please wait..."):
            st.session_state.current_response = requests.post(f"http://{self.server_endpoint}/app/predict", data=query).json()
        time_end = perf_counter()

        current_latency_ms = (time_end - time_start) * 1000
        current_server_latency_ms = st.session_state.current_response['diagnostics']['server_side_latency_ms']
        current_network_latency_ms = current_latency_ms - current_server_latency_ms


        if 'last_latency_ms' in st.session_state:
            st.session_state['last_latency_delta_ms'] = st.session_state.last_latency_ms - current_latency_ms
        else:
            st.session_state['last_latency_delta_ms'] = 0

        if 'last_server_latency_ms' in st.session_state:
            st.session_state['last_server_latency_delta_ms'] = st.session_state.last_server_latency_ms - current_server_latency_ms
        else:
            st.session_state['last_server_latency_delta_ms'] = 0

        if 'last_network_latency_ms' in st.session_state:
            st.session_state['last_network_latency_delta_ms'] = st.session_state.last_network_latency_ms - current_network_latency_ms
        else:
            st.session_state['last_network_latency_delta_ms'] = 0

        st.session_state.last_latency_ms = current_latency_ms
        st.session_state.last_server_latency_ms = current_server_latency_ms
        st.session_state.last_network_latency_ms = current_network_latency_ms

        logging.info(st.session_state.current_response)

    def send_feedback(self):
        feedback = Feedback(text_feedback=st.session_state.text_feedback).json()
        with st.spinner("Thanks for the feedback! Sending feedback to server. Please wait..."):
            requests.post(f"http://{self.server_endpoint}/app/feedback", data=feedback).json()
        st.success('Thanks! Your feedback was received :)')
    
    def send_prediction_feedback(self):
        feedback = PredictionFeedback(query=st.session_state.query, predicted_class=st.session_state.current_response['predicted_class'], user_suggested_class=st.session_state.user_suggested_label).json()
        with st.spinner("Thanks for the feedback! Sending feedback to server. Please wait..."):
            requests.post(f"http://{self.server_endpoint}/app/prediction_feedback", data=feedback).json()
        st.success('Thanks! Your feedback was received :)')

    def request_model_info(self):
        with st.spinner("Requesting model info from server. Please wait..."):
            response = requests.get(f"http://{self.server_endpoint}/app/model_info").json()
            st.session_state.current_model_name = response['model_name']
            st.session_state.current_model_version = response['model_version']
            st.session_state.current_model_test_f1 = response['model_test_f1_score']

if __name__=="__main__":
    # if 'server_endpoint' not in st.session_state:
    parser = argparse.ArgumentParser(description='Run the frontend application.')
    parser.add_argument('--server_endpoint', type=str)
    args = parser.parse_args()

    if not args.server_endpoint:
        args.server_endpoint = "127.0.0.1:8000"
        logging.warning(f"No server endpoint specified, connecting to server at {args.server_endpoint}.")

    StreamlitFrontend(args)